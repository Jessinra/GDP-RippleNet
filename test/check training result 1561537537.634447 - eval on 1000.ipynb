{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <b><i> Test RippleNet Result </b></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# > Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import display\n",
    "from ipywidgets import FloatProgress, IntProgress\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# > Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TEST_CODE = \"1561537537.634447\"\n",
    "CHOSEN_EPOCH = 8\n",
    "\n",
    "MODEL_PATH = \"../log/{}/models/epoch_{}\".format(TEST_CODE, CHOSEN_EPOCH)\n",
    "LOG_PATH = \"../log/{}/log.txt\".format(TEST_CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# > Model src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Limit GPU usage\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Logger.py\n",
    "\n",
    "class Logger:\n",
    "\n",
    "    def set_default_filename(self, filename):\n",
    "        self.default_filename = filename\n",
    "\n",
    "    def create_session_folder(self, path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError:\n",
    "            print(\"Creation of the directory %s failed\" % path)\n",
    "        else:\n",
    "            print(\"\\n ===> Successfully created the directory %s \\n\" % path)\n",
    "\n",
    "    def log(self, text):\n",
    "        with open(self.default_filename, 'a') as f:\n",
    "            f.writelines(text)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    def save_model(self, model, filename):\n",
    "        pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Model.py\n",
    "\n",
    "class RippleNet(object):\n",
    "\n",
    "    def __init__(self, args, n_entity, n_relation):\n",
    "\n",
    "        self._parse_args(args, n_entity, n_relation)\n",
    "        self._build_inputs()\n",
    "        self._build_embeddings()\n",
    "        self._build_model()\n",
    "        self._build_loss()\n",
    "        self._build_train()\n",
    "\n",
    "    def _parse_args(self, args, n_entity, n_relation):\n",
    "\n",
    "        self.n_entity = n_entity\n",
    "        self.n_relation = n_relation\n",
    "        self.dim = args.dim\n",
    "        self.n_hop = args.n_hop\n",
    "        self.kge_weight = args.kge_weight\n",
    "        self.l2_weight = args.l2_weight\n",
    "        self.lr = args.lr\n",
    "        self.n_memory = args.n_memory\n",
    "        self.item_update_mode = args.item_update_mode\n",
    "        self.using_all_hops = args.using_all_hops\n",
    "\n",
    "    def _build_inputs(self):\n",
    "\n",
    "        self.items = tf.placeholder(dtype=tf.int32, shape=[None], name=\"items\")\n",
    "        self.labels = tf.placeholder(dtype=tf.float64, shape=[None], name=\"labels\")\n",
    "        self.memories_h = []\n",
    "        self.memories_r = []\n",
    "        self.memories_t = []\n",
    "\n",
    "        for hop in range(self.n_hop):\n",
    "\n",
    "            self.memories_h.append(\n",
    "                tf.placeholder(dtype=tf.int32, shape=[None, self.n_memory], name=\"memories_h_\" + str(hop)))\n",
    "\n",
    "            self.memories_r.append(\n",
    "                tf.placeholder(dtype=tf.int32, shape=[None, self.n_memory], name=\"memories_r_\" + str(hop)))\n",
    "\n",
    "            self.memories_t.append(\n",
    "                tf.placeholder(dtype=tf.int32, shape=[None, self.n_memory], name=\"memories_t_\" + str(hop)))\n",
    "\n",
    "    def _build_embeddings(self):\n",
    "\n",
    "        self.entity_emb_matrix = tf.get_variable(name=\"entity_emb_matrix\", dtype=tf.float64,\n",
    "                                                 shape=[self.n_entity, self.dim],\n",
    "                                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        self.relation_emb_matrix = tf.get_variable(name=\"relation_emb_matrix\", dtype=tf.float64,\n",
    "                                                   shape=[self.n_relation, self.dim, self.dim],\n",
    "                                                   initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    def _build_model(self):\n",
    "        # transformation matrix for updating item embeddings at the end of each hop\n",
    "        self.transform_matrix = tf.get_variable(name=\"transform_matrix\", shape=[self.dim, self.dim], dtype=tf.float64,\n",
    "                                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        # [batch size, dim]\n",
    "        self.item_embeddings = tf.nn.embedding_lookup(self.entity_emb_matrix, self.items)\n",
    "\n",
    "        self.h_emb_list = []\n",
    "        self.r_emb_list = []\n",
    "        self.t_emb_list = []\n",
    "\n",
    "        for i in range(self.n_hop):\n",
    "\n",
    "            # [batch size, n_memory, dim]\n",
    "            self.h_emb_list.append(tf.nn.embedding_lookup(self.entity_emb_matrix, self.memories_h[i]))\n",
    "\n",
    "            # [batch size, n_memory, dim, dim]\n",
    "            self.r_emb_list.append(tf.nn.embedding_lookup(self.relation_emb_matrix, self.memories_r[i]))\n",
    "\n",
    "            # [batch size, n_memory, dim]\n",
    "            self.t_emb_list.append(tf.nn.embedding_lookup(self.entity_emb_matrix, self.memories_t[i]))\n",
    "\n",
    "        o_list = self._key_addressing()\n",
    "\n",
    "        self.scores = tf.squeeze(self.predict(self.item_embeddings, o_list))\n",
    "        self.scores_normalized = tf.sigmoid(self.scores)\n",
    "\n",
    "    def _key_addressing(self):\n",
    "\n",
    "        o_list = []\n",
    "        for hop in range(self.n_hop):\n",
    "\n",
    "            # [batch_size, n_memory, dim, 1]\n",
    "            h_expanded = tf.expand_dims(self.h_emb_list[hop], axis=3)\n",
    "\n",
    "            # [batch_size, n_memory, dim]\n",
    "            Rh = tf.squeeze(tf.matmul(self.r_emb_list[hop], h_expanded), axis=3)\n",
    "\n",
    "            # [batch_size, dim, 1]\n",
    "            v = tf.expand_dims(self.item_embeddings, axis=2)\n",
    "\n",
    "            # [batch_size, n_memory]\n",
    "            probs = tf.squeeze(tf.matmul(Rh, v), axis=2)\n",
    "\n",
    "            # [batch_size, n_memory]\n",
    "            probs_normalized = tf.nn.softmax(probs)\n",
    "\n",
    "            # [batch_size, n_memory, 1]\n",
    "            probs_expanded = tf.expand_dims(probs_normalized, axis=2)\n",
    "\n",
    "            # [batch_size, dim]\n",
    "            o = tf.reduce_sum(self.t_emb_list[hop] * probs_expanded, axis=1)\n",
    "\n",
    "            self.item_embeddings = self.update_item_embedding(self.item_embeddings, o)\n",
    "            o_list.append(o)\n",
    "\n",
    "        return o_list\n",
    "\n",
    "    def update_item_embedding(self, item_embeddings, o):\n",
    "\n",
    "        if self.item_update_mode == \"replace\":\n",
    "            item_embeddings = o\n",
    "        elif self.item_update_mode == \"plus\":\n",
    "            item_embeddings = item_embeddings + o\n",
    "        elif self.item_update_mode == \"replace_transform\":\n",
    "            item_embeddings = tf.matmul(o, self.transform_matrix)\n",
    "        elif self.item_update_mode == \"plus_transform\":\n",
    "            item_embeddings = tf.matmul(item_embeddings + o, self.transform_matrix)\n",
    "        else:\n",
    "            raise Exception(\"Unknown item updating mode: \" + self.item_update_mode)\n",
    "\n",
    "        return item_embeddings\n",
    "\n",
    "    def predict(self, item_embeddings, o_list):\n",
    "\n",
    "        y = o_list[-1]\n",
    "        if self.using_all_hops:\n",
    "            for i in range(self.n_hop - 1):\n",
    "                y += o_list[i]\n",
    "\n",
    "        scores = tf.reduce_sum(item_embeddings * y, axis=1)\n",
    "        return scores\n",
    "\n",
    "    def _build_loss(self):\n",
    "\n",
    "        self.base_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels, logits=self.scores))\n",
    "\n",
    "        self.kge_loss = 0\n",
    "        for hop in range(self.n_hop):\n",
    "\n",
    "            h_expanded = tf.expand_dims(self.h_emb_list[hop], axis=2)\n",
    "            t_expanded = tf.expand_dims(self.t_emb_list[hop], axis=3)\n",
    "            hRt = tf.squeeze(tf.matmul(tf.matmul(h_expanded, self.r_emb_list[hop]), t_expanded))\n",
    "            self.kge_loss += tf.reduce_mean(tf.sigmoid(hRt))\n",
    "\n",
    "        self.kge_loss = -self.kge_weight * self.kge_loss\n",
    "\n",
    "        self.l2_loss = 0\n",
    "        for hop in range(self.n_hop):\n",
    "\n",
    "            self.l2_loss += tf.reduce_mean(tf.reduce_sum(self.h_emb_list[hop] * self.h_emb_list[hop]))\n",
    "            self.l2_loss += tf.reduce_mean(tf.reduce_sum(self.t_emb_list[hop] * self.t_emb_list[hop]))\n",
    "            self.l2_loss += tf.reduce_mean(tf.reduce_sum(self.r_emb_list[hop] * self.r_emb_list[hop]))\n",
    "            if self.item_update_mode == \"replace nonlinear\" or self.item_update_mode == \"plus nonlinear\":\n",
    "                self.l2_loss += tf.nn.l2_loss(self.transform_matrix)\n",
    "\n",
    "        self.l2_loss = self.l2_weight * self.l2_loss\n",
    "\n",
    "        self.loss = self.base_loss + self.kge_loss + self.l2_loss\n",
    "\n",
    "    def _build_train(self):\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "        '''\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        gradients, variables = zip(*optimizer.compute_gradients(self.loss))\n",
    "        gradients = [None if gradient is None else tf.clip_by_norm(gradient, clip_norm=5)\n",
    "                     for gradient in gradients]\n",
    "        self.optimizer = optimizer.apply_gradients(zip(gradients, variables))\n",
    "        '''\n",
    "\n",
    "    def train(self, sess, feed_dict):\n",
    "        return sess.run([self.optimizer, self.loss], feed_dict)\n",
    "\n",
    "    def eval(self, sess, feed_dict):\n",
    "\n",
    "        labels, scores = sess.run([self.labels, self.scores_normalized], feed_dict)\n",
    "\n",
    "        auc = roc_auc_score(y_true=labels, y_score=scores)\n",
    "\n",
    "        predictions = [1 if i >= 0.5 else 0 for i in scores]\n",
    "        acc = np.mean(np.equal(predictions, labels))\n",
    "\n",
    "        return auc, acc\n",
    "\n",
    "    # ============ Custom test purpose ============\n",
    "    def custom_eval(self, sess, feed_dict):\n",
    "\n",
    "        labels, scores = sess.run([self.labels, self.scores_normalized], feed_dict)\n",
    "        auc = roc_auc_score(y_true=labels, y_score=scores)\n",
    "        predictions = [1 if i >= 0.5 else 0 for i in scores]\n",
    "        acc = np.mean(np.equal(predictions, labels))\n",
    "\n",
    "        return auc, acc, labels, scores, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Dataloader.py\n",
    "\n",
    "def load_data(args):\n",
    "\n",
    "    train_data, eval_data, test_data, user_history_dict = _load_rating(args)\n",
    "    n_entity, n_relation, kg = load_kg(args)\n",
    "    ripple_set = _get_ripple_set(args, kg, user_history_dict)\n",
    "\n",
    "    return train_data, eval_data, test_data, n_entity, n_relation, ripple_set\n",
    "\n",
    "\n",
    "def _load_rating(args):\n",
    "    print('reading rating file ...')\n",
    "\n",
    "    rating_file = '../data/' + args.dataset + '/ratings_final'\n",
    "\n",
    "    if os.path.exists(rating_file + '.npy'):\n",
    "        print(\"loaded from cache : {}.npy\".format(rating_file))\n",
    "        rating_np = np.load(rating_file + '.npy')\n",
    "\n",
    "    else:\n",
    "        rating_np = np.loadtxt(rating_file + '.txt', dtype=np.int32)\n",
    "\n",
    "        print(\"saved to cache : {}.npy\".format(rating_file))\n",
    "        np.save(rating_file + '.npy', rating_np)\n",
    "\n",
    "    return _dataset_split(rating_np, eval_ratio=args.eval_ratio, test_ratio=args.test_ratio)\n",
    "\n",
    "\n",
    "def _dataset_split(rating_np, eval_ratio=0.2, test_ratio=0.2):\n",
    "    print('splitting dataset ...')\n",
    "\n",
    "    n_ratings = rating_np.shape[0]\n",
    "\n",
    "    eval_indices = np.random.choice(n_ratings, size=int(n_ratings * eval_ratio), replace=False)\n",
    "    left = set(range(n_ratings)) - set(eval_indices)\n",
    "\n",
    "    test_indices = np.random.choice(list(left), size=int(n_ratings * test_ratio), replace=False)\n",
    "    train_indices = list(left - set(test_indices))\n",
    "\n",
    "    # traverse training data, only keeping the users with positive ratings\n",
    "    user_history_dict = dict()\n",
    "    for i in train_indices:\n",
    "        user = rating_np[i][0]\n",
    "        item = rating_np[i][1]\n",
    "        rating = rating_np[i][2]\n",
    "        if rating == 1:\n",
    "            if user not in user_history_dict:\n",
    "                user_history_dict[user] = []\n",
    "            user_history_dict[user].append(item)\n",
    "\n",
    "    train_indices = [i for i in train_indices if rating_np[i][0] in user_history_dict]\n",
    "    eval_indices = [i for i in eval_indices if rating_np[i][0] in user_history_dict]\n",
    "    test_indices = [i for i in test_indices if rating_np[i][0] in user_history_dict]\n",
    "\n",
    "    train_data = rating_np[train_indices]\n",
    "    eval_data = rating_np[eval_indices]\n",
    "    test_data = rating_np[test_indices]\n",
    "\n",
    "    return train_data, eval_data, test_data, user_history_dict\n",
    "\n",
    "\n",
    "def load_kg(args):\n",
    "    print('reading KG file ...')\n",
    "\n",
    "    kg_file = '../data/' + args.dataset + '/kg_final'\n",
    "\n",
    "    if os.path.exists(kg_file + '.npy'):\n",
    "        print(\"loaded from cache : {}.npy\".format(kg_file))\n",
    "        kg_numpy = np.load(kg_file + '.npy')\n",
    "\n",
    "    else:\n",
    "        kg_numpy = np.loadtxt(kg_file + '.txt', dtype=np.int32)\n",
    "\n",
    "        print(\"saved to cache : {}.npy\".format(kg_file))\n",
    "        np.save(kg_file + '.npy', kg_numpy)\n",
    "\n",
    "    n_entity = len(set(kg_numpy[:, 0]) | set(kg_numpy[:, 2]))\n",
    "    n_relation = len(set(kg_numpy[:, 1]))\n",
    "\n",
    "    kg = _construct_kg(kg_numpy)\n",
    "\n",
    "    return n_entity, n_relation, kg\n",
    "\n",
    "\n",
    "def _construct_kg(kg_numpy):\n",
    "    print('constructing knowledge graph ...')\n",
    "\n",
    "    kg = collections.defaultdict(list)\n",
    "    for head, relation, tail in kg_numpy:\n",
    "        kg[head].append((tail, relation))\n",
    "\n",
    "    return kg\n",
    "\n",
    "\n",
    "def _get_ripple_set(args, kg, user_history_dict):\n",
    "    print('constructing ripple set ...')\n",
    "\n",
    "    # Creating dictionary with format :\n",
    "    # {user : [(hop_0_heads, hop_0_relations, hop_0_tails), (hop_1_heads, hop_1_relations, hop_1_tails), ...]}\n",
    "\n",
    "    ripple_set = collections.defaultdict(list)\n",
    "    for user in tqdm(user_history_dict):\n",
    "        for h in range(args.n_hop):\n",
    "            memories_h = []\n",
    "            memories_r = []\n",
    "            memories_t = []\n",
    "\n",
    "            if h == 0:\n",
    "                tails_of_last_hop = user_history_dict[user]\n",
    "            else:\n",
    "                tails_of_last_hop = ripple_set[user][-1][2]\n",
    "\n",
    "            for entity in tails_of_last_hop:\n",
    "                for tail_and_relation in kg[entity]:\n",
    "                    memories_h.append(entity)\n",
    "                    memories_r.append(tail_and_relation[1])\n",
    "                    memories_t.append(tail_and_relation[0])\n",
    "\n",
    "            # if the current ripple set of the given user is empty, we simply copy the ripple set of the last hop here\n",
    "            # this won't happen for h = 0, because only the items that appear in the KG have been selected\n",
    "            # this only happens on 154 users in Book-Crossing dataset (since both BX dataset and the KG are sparse)\n",
    "            if len(memories_h) == 0:\n",
    "                ripple_set[user].append(ripple_set[user][-1])\n",
    "\n",
    "            else:\n",
    "                # sample a fixed-size 1-hop memory for each user\n",
    "                replace = len(memories_h) < args.n_memory\n",
    "                indices = np.random.choice(len(memories_h), size=args.n_memory, replace=replace)\n",
    "\n",
    "                memories_h = [memories_h[i] for i in indices]\n",
    "                memories_r = [memories_r[i] for i in indices]\n",
    "                memories_t = [memories_t[i] for i in indices]\n",
    "\n",
    "                ripple_set[user].append((memories_h, memories_r, memories_t))\n",
    "\n",
    "    return ripple_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Train.py\n",
    "\n",
    "timestamp = str(datetime.timestamp(datetime.now()))\n",
    "SESSION_LOG_PATH = \"../log/{}/\".format(timestamp)\n",
    "\n",
    "def train(args, data_info, show_loss, config):\n",
    "\n",
    "    train_data = data_info[0]\n",
    "    eval_data = data_info[1]\n",
    "    test_data = data_info[2]\n",
    "    n_entity = data_info[3]\n",
    "    n_relation = data_info[4]\n",
    "    ripple_set = data_info[5]\n",
    "\n",
    "    logger = Logger()\n",
    "    logger.create_session_folder(SESSION_LOG_PATH)\n",
    "    logger.set_default_filename(SESSION_LOG_PATH + \"log.txt\")\n",
    "    logger.log(str(args))   # Log training and model hyper parameters\n",
    "\n",
    "    model = RippleNet(args, n_entity, n_relation)\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "        for step in range(args.n_epoch):\n",
    "\n",
    "            np.random.shuffle(train_data)\n",
    "\n",
    "            # training\n",
    "            for i in tqdm(range(0, train_data.shape[0], args.batch_size)):\n",
    "\n",
    "                _, loss = model.train(sess, _get_feed_dict(args, model, train_data, ripple_set, i, i + args.batch_size))\n",
    "\n",
    "                if show_loss:\n",
    "                    print('%.1f%% %.4f' % (i / train_data.shape[0] * 100, loss))\n",
    "                    logger.log('%.1f%% %.4f' % (i / train_data.shape[0] * 100, loss))\n",
    "\n",
    "            # evaluation\n",
    "            train_auc, train_acc = _evaluation(sess, args, model, train_data, ripple_set)\n",
    "            eval_auc, eval_acc = _evaluation(sess, args, model, eval_data, ripple_set)\n",
    "            test_auc, test_acc = _evaluation(sess, args, model, test_data, ripple_set)\n",
    "\n",
    "            # Save the variables to disk.\n",
    "            saver.save(sess, SESSION_LOG_PATH + \"models/epoch_{}\".format(step))\n",
    "\n",
    "            print('epoch %d    train auc: %.4f  acc: %.4f    eval auc: %.4f  acc: %.4f    test auc: %.4f  acc: %.4f'\n",
    "                  % (step, train_auc, train_acc, eval_auc, eval_acc, test_auc, test_acc))\n",
    "\n",
    "            logger.log(\n",
    "                'epoch %d    train auc: %.4f  acc: %.4f    eval auc: %.4f  acc: %.4f    test auc: %.4f  acc: %.4f'\n",
    "                % (step, train_auc, train_acc, eval_auc, eval_acc, test_auc, test_acc))\n",
    "\n",
    "\n",
    "def _get_feed_dict(args, model, data, ripple_set, start, end):\n",
    "\n",
    "    feed_dict = dict()\n",
    "    feed_dict[model.items] = data[start:end, 1]\n",
    "    feed_dict[model.labels] = data[start:end, 2]\n",
    "\n",
    "    for i in range(args.n_hop):\n",
    "        feed_dict[model.memories_h[i]] = [ripple_set[user][i][0] for user in data[start:end, 0]]\n",
    "        feed_dict[model.memories_r[i]] = [ripple_set[user][i][1] for user in data[start:end, 0]]\n",
    "        feed_dict[model.memories_t[i]] = [ripple_set[user][i][2] for user in data[start:end, 0]]\n",
    "\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def _evaluation(sess, args, model, eval_data, ripple_set):\n",
    "\n",
    "    auc_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    for i in tqdm(range(0, eval_data.shape[0], args.batch_size)):\n",
    "        auc, acc = model.eval(sess, _get_feed_dict(args, model, eval_data, ripple_set, i, i + args.batch_size))\n",
    "        auc_list.append(auc)\n",
    "        acc_list.append(acc)\n",
    "\n",
    "    return float(np.mean(auc_list)), float(np.mean(acc_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# > Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dataset = 'movie'\n",
    "        self.dim = 16\n",
    "        self.eval_ratio = 0.2\n",
    "        self.test_ratio = 0.2\n",
    "        self.n_hop = 2\n",
    "        self.kge_weight = 0.01\n",
    "        self.l2_weight = 1e-07\n",
    "        self.lr = 0.02\n",
    "        self.batch_size = 1024\n",
    "        self.n_epoch = 10\n",
    "        self.n_memory = 32\n",
    "        self.item_update_mode = 'plus_transform'\n",
    "        self.using_all_hops = True\n",
    "        self.comment = \"running normally\"\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## >> Load cache dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from cache : ../data/movie/preprocessed_data_info_32\n"
     ]
    }
   ],
   "source": [
    "# Main.py\n",
    "\n",
    "cached_preprocessed_data_filename = \"../data/movie/preprocessed_data_info_{}\".format(args.n_memory)\n",
    "\n",
    "# Preprocess data info\n",
    "if os.path.exists(cached_preprocessed_data_filename):\n",
    "    print(\"loaded from cache : {}\".format(cached_preprocessed_data_filename))\n",
    "    data_info = pickle.load(open(cached_preprocessed_data_filename, 'rb'))\n",
    "\n",
    "else:\n",
    "    data_info = load_data(args)\n",
    "\n",
    "    print(\"saved to cache : {}\".format(cached_preprocessed_data_filename))\n",
    "    pickle.dump(data_info, open(cached_preprocessed_data_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Separate the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_info[0]\n",
    "eval_data = data_info[1]\n",
    "test_data = data_info[2]\n",
    "n_entity = data_info[3]\n",
    "n_relation = data_info[4]\n",
    "ripple_set = data_info[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Create truth dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8235631/8235631 [00:28<00:00, 293577.99it/s]\n",
      "100%|██████████| 2744563/2744563 [00:11<00:00, 241905.18it/s]\n",
      "100%|██████████| 2744582/2744582 [00:12<00:00, 220705.46it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_truth_dict():\n",
    "\n",
    "    truth_dict = {}\n",
    "    for rating in tqdm(train_data):\n",
    "        user_id, movie_id, score = rating\n",
    "\n",
    "        if user_id not in truth_dict:\n",
    "            truth_dict[user_id] = []\n",
    "\n",
    "        if score == 1:\n",
    "            truth_dict[user_id].append(movie_id)\n",
    "\n",
    "    for rating in tqdm(test_data):\n",
    "        user_id, movie_id, score = rating\n",
    "\n",
    "        if user_id not in truth_dict:\n",
    "            truth_dict[user_id] = []\n",
    "\n",
    "        if score == 1:\n",
    "            truth_dict[user_id].append(movie_id)\n",
    "\n",
    "    for rating in tqdm(eval_data):\n",
    "        user_id, movie_id, score = rating\n",
    "\n",
    "        if user_id not in truth_dict:\n",
    "            truth_dict[user_id] = []\n",
    "\n",
    "        if score == 1:\n",
    "            truth_dict[user_id].append(movie_id)\n",
    "\n",
    "    return truth_dict\n",
    "\n",
    "\n",
    "truth_dict = generate_truth_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## >> Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0801 12:43:41.303251 140135474120448 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0801 12:43:41.386160 140135474120448 deprecation.py:323] From /home/jessinra/.local/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = RippleNet(args, n_entity, n_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0801 12:43:42.653285 140135474120448 deprecation.py:323] From /home/jessinra/.local/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "saver = tf.train.import_meta_graph(MODEL_PATH + \".meta\")\n",
    "saver.restore(sess, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## >> Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sess, args, model, users, items):\n",
    "\n",
    "    test_data = _preprocess_test_data(users, items)\n",
    "\n",
    "    scores = []\n",
    "    for i in range(0, len(test_data), args.batch_size):\n",
    "\n",
    "        feed_dict = _get_feed_dict(args, model, test_data, ripple_set, i, i + args.batch_size)\n",
    "        _, _, _, batch_scores, _ = model.custom_eval(sess, feed_dict)\n",
    "        scores = np.concatenate((scores, batch_scores))\n",
    "\n",
    "    return scores\n",
    "\n",
    "def _preprocess_test_data(users, items):\n",
    "    \"Preprocess test data so ripplenet can do feed forward with the right format\"\n",
    "    \n",
    "    dummy_value = True\n",
    "\n",
    "    cust_test_data = []\n",
    "    for user in users:\n",
    "        for item in items:\n",
    "            cust_test_data.append([user, item, int(dummy_value)]) # The last dummy value to match input format\n",
    "            dummy_value = not(dummy_value)\n",
    "            \n",
    "    return np.array(cust_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestion(user, k):\n",
    "\n",
    "    items = [i for i in range(0, 15440)]\n",
    "    prediction = predict(sess, args, model, [user], items)\n",
    "    score_item_pairs = [(prediction[i], i) for i in items]\n",
    "    top_k_recommendations = sorted(score_item_pairs, reverse=True)[:k]\n",
    "\n",
    "    return top_k_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_truth(user, k):\n",
    "    return truth_dict[user] if user in truth_dict else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_precision(prediction, truth, k=10):\n",
    "    intersect = _get_intersect_pred_truth(prediction, truth, k)\n",
    "    len_intersect = len(intersect)\n",
    "    len_truth = len(truth) if 0 < len(truth) <= k else k\n",
    "\n",
    "    return intersect, len_intersect / len_truth\n",
    "\n",
    "def _get_intersect_pred_truth(pred, truth, k):\n",
    "    pred_item_set = {x[1] for x in pred}\n",
    "    truth_item_set = set(truth)\n",
    "\n",
    "    return pred_item_set.intersection(truth_item_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 271/1000 [03:36<09:39,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occur for 1079 : list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 328/1000 [04:23<08:18,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occur for 1723 : list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 461/1000 [06:01<06:42,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occur for 5267 : list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 600/1000 [07:48<05:02,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occur for 5905 : list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 625/1000 [08:09<05:21,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error occur for 8662 : list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [12:54<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "k_suggestion = 10\n",
    "n_users = 1000\n",
    "\n",
    "sample_user = np.random.randint(1, 15000, n_users) # sampling\n",
    "# sample_user = [i in range(0, 15000)] # uncomment to use non sampling\n",
    "\n",
    "suggested_items = []\n",
    "truth_items = []\n",
    "intersects = []\n",
    "scores = []\n",
    "\n",
    "all_intersect = None\n",
    "all_union = None\n",
    "\n",
    "for user in tqdm(sample_user):\n",
    "\n",
    "    try:\n",
    "\n",
    "        top_suggestions = get_suggestion(user, k_suggestion)\n",
    "        top_suggested_items = set([x[1] for x in top_suggestions])\n",
    "        top_truth_items = get_top_truth(user, k_suggestion)\n",
    "\n",
    "        intersect, score = check_precision(top_suggestions, top_truth_items, k=k_suggestion)\n",
    "\n",
    "        suggested_items.append(top_suggested_items)\n",
    "        truth_items.append(top_truth_items)\n",
    "        intersects.append(intersect)\n",
    "        scores.append(score)\n",
    "\n",
    "        if all_intersect is None:\n",
    "            all_intersect = top_suggested_items\n",
    "        else:\n",
    "            all_intersect = all_intersect.intersection(top_suggested_items)\n",
    "\n",
    "        if all_union is None:\n",
    "            all_union = top_suggested_items\n",
    "        else:\n",
    "            all_union = all_union.union(top_suggested_items)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error occur for {} : {}\".format(user, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prec@k score: 0.2707398101619207\n",
      "\n",
      "intersect\n",
      "set() 0\n",
      "\n",
      "union\n",
      "{7170, 11268, 9224, 9226, 9227, 10252, 20, 1048, 2073, 2074, 10268, 10272, 2085, 5161, 11309, 2093, 7219, 3124, 12341, 12342, 12339, 12351, 3137, 12354, 10306, 2114, 4165, 11334, 6215, 14408, 8266, 1101, 2130, 11347, 5209, 4195, 1125, 8293, 1138, 7285, 122, 5246, 3200, 10371, 6276, 7304, 141, 10382, 1168, 7313, 12436, 4248, 5276, 4262, 9383, 12456, 12461, 7347, 9396, 6325, 8377, 4284, 1216, 3264, 2243, 14532, 12489, 8397, 10455, 6362, 5339, 13533, 8421, 11494, 8422, 3307, 7417, 7423, 12545, 14596, 8454, 9481, 5387, 8460, 7442, 4370, 8474, 10524, 5409, 2339, 1318, 6439, 1321, 1322, 11564, 1324, 5425, 5439, 9536, 14657, 11584, 7489, 6469, 9544, 3401, 11600, 1361, 13650, 340, 12637, 11617, 7526, 4457, 8560, 1392, 1402, 3450, 14719, 11648, 10624, 2434, 8579, 8583, 4487, 6546, 10644, 9620, 2455, 3482, 1435, 9630, 7586, 10659, 10660, 4520, 11690, 12715, 4526, 1459, 3508, 3515, 4541, 9662, 13766, 4551, 12743, 2506, 8652, 6605, 4557, 7629, 2517, 477, 12766, 11743, 8672, 3551, 6623, 4583, 9704, 13799, 9706, 491, 11757, 8688, 2551, 6648, 4601, 5629, 10750, 11775, 12799, 11777, 14848, 8714, 3603, 533, 534, 1559, 8729, 9758, 8734, 9763, 1572, 11823, 13872, 7727, 7730, 11832, 12860, 1598, 6727, 14920, 2633, 14923, 9806, 4687, 8783, 13910, 8793, 616, 1640, 5738, 12909, 13933, 7794, 5746, 9844, 8820, 629, 7804, 2691, 14980, 11912, 2700, 656, 11924, 13975, 6818, 5795, 676, 9896, 6825, 8878, 10927, 693, 2741, 4791, 14007, 5817, 2744, 700, 7870, 8896, 5826, 709, 14025, 15049, 11979, 15053, 2767, 3794, 6873, 15073, 7907, 2790, 3815, 1772, 7917, 8946, 13043, 4854, 5880, 1785, 11002, 11006, 8962, 2818, 15110, 2829, 14094, 3855, 10002, 1811, 3860, 3870, 2863, 6962, 15154, 5939, 3896, 832, 13122, 15171, 10055, 12107, 8012, 12113, 13140, 854, 12126, 12127, 5987, 3943, 8053, 886, 7036, 15231, 6018, 7042, 15237, 15238, 8074, 911, 2959, 1938, 14231, 9113, 6050, 7076, 15271, 11177, 7083, 11181, 4031, 5055, 3009, 5058, 9157, 7120, 7124, 11224, 10201, 15323, 5086, 11233, 9185, 2032, 4080, 11263} 320\n",
      "\n",
      "distinct rate\n",
      "0.032\n"
     ]
    }
   ],
   "source": [
    "print(\"Prec@k score:\", np.average(scores))\n",
    "# print(\"top_suggested_items:\", top_suggested_items)\n",
    "# print(\"truth_items:\", truth_items)\n",
    "\n",
    "print(\"\\nintersect\")\n",
    "print(all_intersect, len(all_intersect))\n",
    "print(\"\\nunion\")\n",
    "print(all_union, len(all_union))\n",
    "print(\"\\ndistinct rate\")\n",
    "print((len(all_union)) / (n_users * k_suggestion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7596"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0.9948102998663044, 6648),\n",
       " (0.9941328646276528, 2517),\n",
       " (0.9930774190635847, 8793),\n",
       " (0.992843852711498, 9704),\n",
       " (0.9926080346767998, 2551),\n",
       " (0.9908399475329968, 12354),\n",
       " (0.9894365816532724, 5939),\n",
       " (0.9893393865907427, 2500),\n",
       " (0.9889735442656425, 11924),\n",
       " (0.9887048244040225, 9706)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[6648, 2517, 8793, 9704, 2551, 12354, 5939, 2500, 11924, 9706]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[6164,\n",
       " 6167,\n",
       " 36,\n",
       " 10281,\n",
       " 10811,\n",
       " 4670,\n",
       " 8766,\n",
       " 6215,\n",
       " 2638,\n",
       " 4687,\n",
       " 3153,\n",
       " 1619,\n",
       " 2134,\n",
       " 8793,\n",
       " 3163,\n",
       " 10848,\n",
       " 6246,\n",
       " 616,\n",
       " 2154,\n",
       " 11892,\n",
       " 11893,\n",
       " 4727,\n",
       " 7804,\n",
       " 3200,\n",
       " 9347,\n",
       " 13975,\n",
       " 12951,\n",
       " 2201,\n",
       " 11979,\n",
       " 6345,\n",
       " 14027,\n",
       " 4821,\n",
       " 14040,\n",
       " 6873,\n",
       " 2274,\n",
       " 15075,\n",
       " 1785,\n",
       " 6919,\n",
       " 1298,\n",
       " 6423,\n",
       " 5937,\n",
       " 4402,\n",
       " 5939,\n",
       " 10555,\n",
       " 9544,\n",
       " 6496,\n",
       " 7010,\n",
       " 10091,\n",
       " 8559,\n",
       " 10096,\n",
       " 7541,\n",
       " 375,\n",
       " 11648,\n",
       " 9620,\n",
       " 1431,\n",
       " 1955,\n",
       " 10148,\n",
       " 1457,\n",
       " 7624,\n",
       " 7625,\n",
       " 2517,\n",
       " 8660,\n",
       " 11224,\n",
       " 3545,\n",
       " 15323,\n",
       " 8166,\n",
       " 10228,\n",
       " 2551,\n",
       " 6648,\n",
       " 2941,\n",
       " 2671,\n",
       " 13741,\n",
       " 12354,\n",
       " 2500,\n",
       " 2672,\n",
       " 10317,\n",
       " 2639,\n",
       " 7826,\n",
       " 12348,\n",
       " 501,\n",
       " 4307,\n",
       " 10719,\n",
       " 10552,\n",
       " 14529,\n",
       " 14348,\n",
       " 3495,\n",
       " 1745,\n",
       " 14402,\n",
       " 7873,\n",
       " 1392,\n",
       " 13661,\n",
       " 5935,\n",
       " 14026,\n",
       " 14280,\n",
       " 5869,\n",
       " 7959,\n",
       " 5936,\n",
       " 14025,\n",
       " 7400,\n",
       " 5892,\n",
       " 12637,\n",
       " 11895,\n",
       " 5934,\n",
       " 5086,\n",
       " 9122,\n",
       " 141,\n",
       " 4820,\n",
       " 12787,\n",
       " 6317,\n",
       " 11903,\n",
       " 11757,\n",
       " 8653,\n",
       " 989,\n",
       " 4791,\n",
       " 6337,\n",
       " 2327,\n",
       " 10466,\n",
       " 1630,\n",
       " 2004,\n",
       " 2850,\n",
       " 3414,\n",
       " 15133,\n",
       " 14516,\n",
       " 6452]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "({2500, 2517, 2551, 5939, 6648, 8793, 12354}, 0.7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=================='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "85169"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0.9994540972195599, 2073),\n",
       " (0.999369161706882, 1321),\n",
       " (0.9992506115303117, 3401),\n",
       " (0.999098966433822, 911),\n",
       " (0.9990643812669123, 13122),\n",
       " (0.9989946589853846, 10659),\n",
       " (0.9989716772631991, 491),\n",
       " (0.998952045690747, 5738),\n",
       " (0.9988424004477492, 13043),\n",
       " (0.998805799925913, 5058)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[2073, 1321, 3401, 911, 13122, 10659, 491, 5738, 13043, 5058]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[7304,\n",
       " 11536,\n",
       " 9371,\n",
       " 7715,\n",
       " 12342,\n",
       " 6201,\n",
       " 15161,\n",
       " 13122,\n",
       " 2897,\n",
       " 14048,\n",
       " 11762,\n",
       " 2942,\n",
       " 14719,\n",
       " 8925,\n",
       " 375,\n",
       " 3732,\n",
       " 5058,\n",
       " 10659,\n",
       " 1125,\n",
       " 11677,\n",
       " 14320,\n",
       " 2909,\n",
       " 1559]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "({5058, 10659, 13122}, 0.3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=================='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "20487"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0.9911708190085617, 491),\n",
       " (0.988806367237556, 2073),\n",
       " (0.9886556007751237, 14920),\n",
       " (0.9875659842589616, 3401),\n",
       " (0.987119907968069, 1125),\n",
       " (0.9861229810173586, 1321),\n",
       " (0.9860360737934935, 5425),\n",
       " (0.9858199343647869, 6018),\n",
       " (0.9843245785372913, 10659),\n",
       " (0.9839544433879563, 911)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[491, 2073, 14920, 3401, 1125, 1321, 5425, 6018, 10659, 911]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[3264,\n",
       " 5058,\n",
       " 10309,\n",
       " 7304,\n",
       " 1321,\n",
       " 2897,\n",
       " 14162,\n",
       " 7285,\n",
       " 491,\n",
       " 10002,\n",
       " 911,\n",
       " 11536,\n",
       " 340,\n",
       " 5409,\n",
       " 6796]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "({491, 911, 1321}, 0.3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=================='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_user = [np.random.randint(1, 138000) for i in range(0, 3)]\n",
    "\n",
    "for user in sample_user:\n",
    "\n",
    "    prediction = get_suggestion(user, 10)\n",
    "    truth = get_top_truth(user, 10)\n",
    "\n",
    "    display(user)\n",
    "    display((prediction))\n",
    "    display([x[1] for x in prediction])\n",
    "    display((truth))\n",
    "    display(check_precision(prediction, truth, 10))\n",
    "    display(\"==================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
