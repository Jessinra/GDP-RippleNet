{"cells":[{"source":"# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\n# ms-python.python added\nimport os\ntry:\n\tos.chdir(os.path.join(os.getcwd(), '..'))\n\tprint(os.getcwd())\nexcept:\n\tpass\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["  # Test RippleNet Result"],"metadata":{}},{"source":["\n","import argparse\n","import collections\n","import os\n","import pickle\n","from datetime import datetime\n","\n","import numpy as np\n","import tensorflow as tf\n","from IPython.display import display\n","from ipywidgets import FloatProgress, IntProgress\n","from sklearn.metrics import roc_auc_score\n","from tqdm import tqdm\n","\n",""],"cell_type":"code","outputs":[{"output_type":"stream","name":"stderr","text":"/home/jessinra/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/home/jessinra/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/home/jessinra/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/home/jessinra/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/home/jessinra/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/home/jessinra/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n/home/jessinra/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/home/jessinra/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/home/jessinra/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/home/jessinra/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/home/jessinra/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/home/jessinra/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"}],"metadata":{},"execution_count":1},{"source":["\n","\n","TEST_CODE = \"1561537537.634447\"\n","CHOSEN_EPOCH = 8\n","\n","MODEL_PATH = \"../log/{}/models/epoch_{}\".format(TEST_CODE, CHOSEN_EPOCH)\n","LOG_PATH = \"../log/{}/log.txt\".format(TEST_CODE)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":2},{"source":["\n","\n","np.random.seed(555)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":3},{"source":["\n","\n","# Limit GPU usage\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":4},{"source":["\n","\n","# Logger.py\n","\n","\n","class Logger:\n","\n","    def set_default_filename(self, filename):\n","        self.default_filename = filename\n","\n","    def create_session_folder(self, path):\n","        try:\n","            os.makedirs(path)\n","        except OSError:\n","            print(\"Creation of the directory %s failed\" % path)\n","        else:\n","            print(\"\\n ===> Successfully created the directory %s \\n\" % path)\n","\n","    def log(self, text):\n","        with open(self.default_filename, 'a') as f:\n","            f.writelines(text)\n","            f.write(\"\\n\")\n","\n","    def save_model(self, model, filename):\n","        pickle.dump(model, open(filename, 'wb'))\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":5},{"source":["\n","\n","# Model.py\n","\n","\n","class RippleNet(object):\n","\n","    def __init__(self, args, n_entity, n_relation):\n","\n","        self._parse_args(args, n_entity, n_relation)\n","        self._build_inputs()\n","        self._build_embeddings()\n","        self._build_model()\n","        self._build_loss()\n","        self._build_train()\n","\n","    def _parse_args(self, args, n_entity, n_relation):\n","\n","        self.n_entity = n_entity\n","        self.n_relation = n_relation\n","        self.dim = args.dim\n","        self.n_hop = args.n_hop\n","        self.kge_weight = args.kge_weight\n","        self.l2_weight = args.l2_weight\n","        self.lr = args.lr\n","        self.n_memory = args.n_memory\n","        self.item_update_mode = args.item_update_mode\n","        self.using_all_hops = args.using_all_hops\n","\n","    def _build_inputs(self):\n","\n","        self.items = tf.placeholder(dtype=tf.int32, shape=[None], name=\"items\")\n","        self.labels = tf.placeholder(dtype=tf.float64, shape=[None], name=\"labels\")\n","        self.memories_h = []\n","        self.memories_r = []\n","        self.memories_t = []\n","\n","        for hop in range(self.n_hop):\n","\n","            self.memories_h.append(\n","                tf.placeholder(dtype=tf.int32, shape=[None, self.n_memory], name=\"memories_h_\" + str(hop)))\n","\n","            self.memories_r.append(\n","                tf.placeholder(dtype=tf.int32, shape=[None, self.n_memory], name=\"memories_r_\" + str(hop)))\n","\n","            self.memories_t.append(\n","                tf.placeholder(dtype=tf.int32, shape=[None, self.n_memory], name=\"memories_t_\" + str(hop)))\n","\n","    def _build_embeddings(self):\n","\n","        self.entity_emb_matrix = tf.get_variable(name=\"entity_emb_matrix\", dtype=tf.float64,\n","                                                 shape=[self.n_entity, self.dim],\n","                                                 initializer=tf.contrib.layers.xavier_initializer())\n","\n","        self.relation_emb_matrix = tf.get_variable(name=\"relation_emb_matrix\", dtype=tf.float64,\n","                                                   shape=[self.n_relation, self.dim, self.dim],\n","                                                   initializer=tf.contrib.layers.xavier_initializer())\n","\n","    def _build_model(self):\n","        # transformation matrix for updating item embeddings at the end of each hop\n","        self.transform_matrix = tf.get_variable(name=\"transform_matrix\", shape=[self.dim, self.dim], dtype=tf.float64,\n","                                                initializer=tf.contrib.layers.xavier_initializer())\n","\n","        # [batch size, dim]\n","        self.item_embeddings = tf.nn.embedding_lookup(self.entity_emb_matrix, self.items)\n","\n","        self.h_emb_list = []\n","        self.r_emb_list = []\n","        self.t_emb_list = []\n","\n","        for i in range(self.n_hop):\n","\n","            # [batch size, n_memory, dim]\n","            self.h_emb_list.append(tf.nn.embedding_lookup(self.entity_emb_matrix, self.memories_h[i]))\n","\n","            # [batch size, n_memory, dim, dim]\n","            self.r_emb_list.append(tf.nn.embedding_lookup(self.relation_emb_matrix, self.memories_r[i]))\n","\n","            # [batch size, n_memory, dim]\n","            self.t_emb_list.append(tf.nn.embedding_lookup(self.entity_emb_matrix, self.memories_t[i]))\n","\n","        o_list = self._key_addressing()\n","\n","        self.scores = tf.squeeze(self.predict(self.item_embeddings, o_list))\n","        self.scores_normalized = tf.sigmoid(self.scores)\n","\n","    def _key_addressing(self):\n","\n","        o_list = []\n","        for hop in range(self.n_hop):\n","\n","            # [batch_size, n_memory, dim, 1]\n","            h_expanded = tf.expand_dims(self.h_emb_list[hop], axis=3)\n","\n","            # [batch_size, n_memory, dim]\n","            Rh = tf.squeeze(tf.matmul(self.r_emb_list[hop], h_expanded), axis=3)\n","\n","            # [batch_size, dim, 1]\n","            v = tf.expand_dims(self.item_embeddings, axis=2)\n","\n","            # [batch_size, n_memory]\n","            probs = tf.squeeze(tf.matmul(Rh, v), axis=2)\n","\n","            # [batch_size, n_memory]\n","            probs_normalized = tf.nn.softmax(probs)\n","\n","            # [batch_size, n_memory, 1]\n","            probs_expanded = tf.expand_dims(probs_normalized, axis=2)\n","\n","            # [batch_size, dim]\n","            o = tf.reduce_sum(self.t_emb_list[hop] * probs_expanded, axis=1)\n","\n","            self.item_embeddings = self.update_item_embedding(self.item_embeddings, o)\n","            o_list.append(o)\n","\n","        return o_list\n","\n","    def update_item_embedding(self, item_embeddings, o):\n","\n","        if self.item_update_mode == \"replace\":\n","            item_embeddings = o\n","        elif self.item_update_mode == \"plus\":\n","            item_embeddings = item_embeddings + o\n","        elif self.item_update_mode == \"replace_transform\":\n","            item_embeddings = tf.matmul(o, self.transform_matrix)\n","        elif self.item_update_mode == \"plus_transform\":\n","            item_embeddings = tf.matmul(item_embeddings + o, self.transform_matrix)\n","        else:\n","            raise Exception(\"Unknown item updating mode: \" + self.item_update_mode)\n","\n","        return item_embeddings\n","\n","    def predict(self, item_embeddings, o_list):\n","\n","        y = o_list[-1]\n","        if self.using_all_hops:\n","            for i in range(self.n_hop - 1):\n","                y += o_list[i]\n","\n","        scores = tf.reduce_sum(item_embeddings * y, axis=1)\n","        return scores\n","\n","    def _build_loss(self):\n","\n","        self.base_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels, logits=self.scores))\n","\n","        self.kge_loss = 0\n","        for hop in range(self.n_hop):\n","\n","            h_expanded = tf.expand_dims(self.h_emb_list[hop], axis=2)\n","            t_expanded = tf.expand_dims(self.t_emb_list[hop], axis=3)\n","            hRt = tf.squeeze(tf.matmul(tf.matmul(h_expanded, self.r_emb_list[hop]), t_expanded))\n","            self.kge_loss += tf.reduce_mean(tf.sigmoid(hRt))\n","\n","        self.kge_loss = -self.kge_weight * self.kge_loss\n","\n","        self.l2_loss = 0\n","        for hop in range(self.n_hop):\n","\n","            self.l2_loss += tf.reduce_mean(tf.reduce_sum(self.h_emb_list[hop] * self.h_emb_list[hop]))\n","            self.l2_loss += tf.reduce_mean(tf.reduce_sum(self.t_emb_list[hop] * self.t_emb_list[hop]))\n","            self.l2_loss += tf.reduce_mean(tf.reduce_sum(self.r_emb_list[hop] * self.r_emb_list[hop]))\n","            if self.item_update_mode == \"replace nonlinear\" or self.item_update_mode == \"plus nonlinear\":\n","                self.l2_loss += tf.nn.l2_loss(self.transform_matrix)\n","\n","        self.l2_loss = self.l2_weight * self.l2_loss\n","\n","        self.loss = self.base_loss + self.kge_loss + self.l2_loss\n","\n","    def _build_train(self):\n","        self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n","        '''\n","        optimizer = tf.train.AdamOptimizer(self.lr)\n","        gradients, variables = zip(*optimizer.compute_gradients(self.loss))\n","        gradients = [None if gradient is None else tf.clip_by_norm(gradient, clip_norm=5)\n","                     for gradient in gradients]\n","        self.optimizer = optimizer.apply_gradients(zip(gradients, variables))\n","        '''\n","\n","    def train(self, sess, feed_dict):\n","        return sess.run([self.optimizer, self.loss], feed_dict)\n","\n","    def eval(self, sess, feed_dict):\n","\n","        labels, scores = sess.run([self.labels, self.scores_normalized], feed_dict)\n","\n","        auc = roc_auc_score(y_true=labels, y_score=scores)\n","\n","        predictions = [1 if i >= 0.5 else 0 for i in scores]\n","        acc = np.mean(np.equal(predictions, labels))\n","\n","        return auc, acc\n","\n","    # ============ Custom test purpose ============\n","    def custom_eval(self, sess, feed_dict):\n","\n","        labels, scores = sess.run([self.labels, self.scores_normalized], feed_dict)\n","        auc = roc_auc_score(y_true=labels, y_score=scores)\n","        predictions = [1 if i >= 0.5 else 0 for i in scores]\n","        acc = np.mean(np.equal(predictions, labels))\n","\n","        return auc, acc, labels, scores, predictions\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":6},{"source":["\n","\n","# Dataloader.py\n","\n","\n","def load_data(args):\n","\n","    train_data, eval_data, test_data, user_history_dict = _load_rating(args)\n","    n_entity, n_relation, kg = load_kg(args)\n","    ripple_set = _get_ripple_set(args, kg, user_history_dict)\n","\n","    return train_data, eval_data, test_data, n_entity, n_relation, ripple_set\n","\n","\n","def _load_rating(args):\n","    print('reading rating file ...')\n","\n","    rating_file = '../data/' + args.dataset + '/ratings_final'\n","\n","    if os.path.exists(rating_file + '.npy'):\n","        print(\"loaded from cache : {}.npy\".format(rating_file))\n","        rating_np = np.load(rating_file + '.npy')\n","\n","    else:\n","        rating_np = np.loadtxt(rating_file + '.txt', dtype=np.int32)\n","\n","        print(\"saved to cache : {}.npy\".format(rating_file))\n","        np.save(rating_file + '.npy', rating_np)\n","\n","    return _dataset_split(rating_np, eval_ratio=args.eval_ratio, test_ratio=args.test_ratio)\n","\n","\n","def _dataset_split(rating_np, eval_ratio=0.2, test_ratio=0.2):\n","    print('splitting dataset ...')\n","\n","    n_ratings = rating_np.shape[0]\n","\n","    eval_indices = np.random.choice(n_ratings, size=int(n_ratings * eval_ratio), replace=False)\n","    left = set(range(n_ratings)) - set(eval_indices)\n","\n","    test_indices = np.random.choice(list(left), size=int(n_ratings * test_ratio), replace=False)\n","    train_indices = list(left - set(test_indices))\n","\n","    # traverse training data, only keeping the users with positive ratings\n","    user_history_dict = dict()\n","    for i in train_indices:\n","        user = rating_np[i][0]\n","        item = rating_np[i][1]\n","        rating = rating_np[i][2]\n","        if rating == 1:\n","            if user not in user_history_dict:\n","                user_history_dict[user] = []\n","            user_history_dict[user].append(item)\n","\n","    train_indices = [i for i in train_indices if rating_np[i][0] in user_history_dict]\n","    eval_indices = [i for i in eval_indices if rating_np[i][0] in user_history_dict]\n","    test_indices = [i for i in test_indices if rating_np[i][0] in user_history_dict]\n","\n","    train_data = rating_np[train_indices]\n","    eval_data = rating_np[eval_indices]\n","    test_data = rating_np[test_indices]\n","\n","    return train_data, eval_data, test_data, user_history_dict\n","\n","\n","def load_kg(args):\n","    print('reading KG file ...')\n","\n","    kg_file = '../data/' + args.dataset + '/kg_final'\n","\n","    if os.path.exists(kg_file + '.npy'):\n","        print(\"loaded from cache : {}.npy\".format(kg_file))\n","        kg_numpy = np.load(kg_file + '.npy')\n","\n","    else:\n","        kg_numpy = np.loadtxt(kg_file + '.txt', dtype=np.int32)\n","\n","        print(\"saved to cache : {}.npy\".format(kg_file))\n","        np.save(kg_file + '.npy', kg_numpy)\n","\n","    n_entity = len(set(kg_numpy[:, 0]) | set(kg_numpy[:, 2]))\n","    n_relation = len(set(kg_numpy[:, 1]))\n","\n","    kg = _construct_kg(kg_numpy)\n","\n","    return n_entity, n_relation, kg\n","\n","\n","def _construct_kg(kg_numpy):\n","    print('constructing knowledge graph ...')\n","\n","    kg = collections.defaultdict(list)\n","    for head, relation, tail in kg_numpy:\n","        kg[head].append((tail, relation))\n","\n","    return kg\n","\n","\n","def _get_ripple_set(args, kg, user_history_dict):\n","    print('constructing ripple set ...')\n","\n","    # Creating dictionary with format :\n","    # {user : [(hop_0_heads, hop_0_relations, hop_0_tails), (hop_1_heads, hop_1_relations, hop_1_tails), ...]}\n","\n","    ripple_set = collections.defaultdict(list)\n","    for user in tqdm(user_history_dict):\n","        for h in range(args.n_hop):\n","            memories_h = []\n","            memories_r = []\n","            memories_t = []\n","\n","            if h == 0:\n","                tails_of_last_hop = user_history_dict[user]\n","            else:\n","                tails_of_last_hop = ripple_set[user][-1][2]\n","\n","            for entity in tails_of_last_hop:\n","                for tail_and_relation in kg[entity]:\n","                    memories_h.append(entity)\n","                    memories_r.append(tail_and_relation[1])\n","                    memories_t.append(tail_and_relation[0])\n","\n","            # if the current ripple set of the given user is empty, we simply copy the ripple set of the last hop here\n","            # this won't happen for h = 0, because only the items that appear in the KG have been selected\n","            # this only happens on 154 users in Book-Crossing dataset (since both BX dataset and the KG are sparse)\n","            if len(memories_h) == 0:\n","                ripple_set[user].append(ripple_set[user][-1])\n","\n","            else:\n","                # sample a fixed-size 1-hop memory for each user\n","                replace = len(memories_h) < args.n_memory\n","                indices = np.random.choice(len(memories_h), size=args.n_memory, replace=replace)\n","\n","                memories_h = [memories_h[i] for i in indices]\n","                memories_r = [memories_r[i] for i in indices]\n","                memories_t = [memories_t[i] for i in indices]\n","\n","                ripple_set[user].append((memories_h, memories_r, memories_t))\n","\n","    return ripple_set\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":7},{"source":["\n","\n","# Train.py\n","\n","\n","timestamp = str(datetime.timestamp(datetime.now()))\n","SESSION_LOG_PATH = \"../log/{}/\".format(timestamp)\n","\n","\n","def train(args, data_info, show_loss, config):\n","\n","    train_data = data_info[0]\n","    eval_data = data_info[1]\n","    test_data = data_info[2]\n","    n_entity = data_info[3]\n","    n_relation = data_info[4]\n","    ripple_set = data_info[5]\n","\n","    logger = Logger()\n","    logger.create_session_folder(SESSION_LOG_PATH)\n","    logger.set_default_filename(SESSION_LOG_PATH + \"log.txt\")\n","    logger.log(str(args))   # Log training and model hyper parameters\n","\n","    model = RippleNet(args, n_entity, n_relation)\n","\n","    with tf.Session(config=config) as sess:\n","        sess.run(tf.global_variables_initializer())\n","        saver = tf.train.Saver(max_to_keep=None)\n","\n","        for step in range(args.n_epoch):\n","\n","            np.random.shuffle(train_data)\n","\n","            # training\n","            for i in tqdm(range(0, train_data.shape[0], args.batch_size)):\n","\n","                _, loss = model.train(sess, _get_feed_dict(args, model, train_data, ripple_set, i, i + args.batch_size))\n","\n","                if show_loss:\n","                    print('%.1f%% %.4f' % (i / train_data.shape[0] * 100, loss))\n","                    logger.log('%.1f%% %.4f' % (i / train_data.shape[0] * 100, loss))\n","\n","            # evaluation\n","            train_auc, train_acc = _evaluation(sess, args, model, train_data, ripple_set)\n","            eval_auc, eval_acc = _evaluation(sess, args, model, eval_data, ripple_set)\n","            test_auc, test_acc = _evaluation(sess, args, model, test_data, ripple_set)\n","\n","            # Save the variables to disk.\n","            saver.save(sess, SESSION_LOG_PATH + \"models/epoch_{}\".format(step))\n","\n","            print('epoch %d    train auc: %.4f  acc: %.4f    eval auc: %.4f  acc: %.4f    test auc: %.4f  acc: %.4f'\n","                  % (step, train_auc, train_acc, eval_auc, eval_acc, test_auc, test_acc))\n","\n","            logger.log(\n","                'epoch %d    train auc: %.4f  acc: %.4f    eval auc: %.4f  acc: %.4f    test auc: %.4f  acc: %.4f'\n","                % (step, train_auc, train_acc, eval_auc, eval_acc, test_auc, test_acc))\n","\n","\n","def _get_feed_dict(args, model, data, ripple_set, start, end):\n","\n","    feed_dict = dict()\n","    feed_dict[model.items] = data[start:end, 1]\n","    feed_dict[model.labels] = data[start:end, 2]\n","\n","    for i in range(args.n_hop):\n","        feed_dict[model.memories_h[i]] = [ripple_set[user][i][0] for user in data[start:end, 0]]\n","        feed_dict[model.memories_r[i]] = [ripple_set[user][i][1] for user in data[start:end, 0]]\n","        feed_dict[model.memories_t[i]] = [ripple_set[user][i][2] for user in data[start:end, 0]]\n","\n","    return feed_dict\n","\n","\n","def _evaluation(sess, args, model, eval_data, ripple_set):\n","\n","    auc_list = []\n","    acc_list = []\n","\n","    for i in tqdm(range(0, eval_data.shape[0], args.batch_size)):\n","        auc, acc = model.eval(sess, _get_feed_dict(args, model, eval_data, ripple_set, i, i + args.batch_size))\n","        auc_list.append(auc)\n","        acc_list.append(acc)\n","\n","    return float(np.mean(auc_list)), float(np.mean(acc_list))\n","\n","\n","# # Args\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":8},{"source":["\n","\n","class Args:\n","\n","    def __init__(self):\n","        self.dataset = 'movie'\n","        self.dim = 16\n","        self.eval_ratio = 0.2\n","        self.test_ratio = 0.2\n","        self.n_hop = 2\n","        self.kge_weight = 0.01\n","        self.l2_weight = 1e-07\n","        self.lr = 0.02\n","        self.batch_size = 1024\n","        self.n_epoch = 10\n","        self.n_memory = 32\n","        self.item_update_mode = 'plus_transform'\n","        self.using_all_hops = True\n","        self.comment = \"running normally\"\n","\n","\n","args = Args()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":9},{"cell_type":"markdown","source":[" ## Load the knowledge graph"],"metadata":{}},{"source":["\n","\n","# Main.py\n","\n","cached_preprocessed_data_filename = \"../data/movie/preprocessed_data_info_{}\".format(args.n_memory)\n","\n","# Preprocess data info\n","if os.path.exists(cached_preprocessed_data_filename):\n","    print(\"loaded from cache : {}\".format(cached_preprocessed_data_filename))\n","    data_info = pickle.load(open(cached_preprocessed_data_filename, 'rb'))\n","\n","else:\n","    data_info = load_data(args)\n","\n","    print(\"saved to cache : {}\".format(cached_preprocessed_data_filename))\n","    pickle.dump(data_info, open(cached_preprocessed_data_filename, 'wb'))\n","\n","train(args, data_info=data_info, show_loss=False, config=config)\n",""],"cell_type":"code","outputs":[{"output_type":"stream","name":"stdout","text":"reading rating file ...\n"},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../data/movie/ratings_final.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-fea510a346a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdata_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved to cache : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_preprocessed_data_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-3e2602fe1497>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_history_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_rating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mn_entity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_relation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_kg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mripple_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_ripple_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_history_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-3e2602fe1497>\u001b[0m in \u001b[0;36m_load_rating\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mrating_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrating_file\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved to cache : {}.npy\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrating_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m             return _file_openers[ext](found, mode=mode,\n\u001b[0;32m--> 621\u001b[0;31m                                       encoding=encoding, newline=newline)\n\u001b[0m\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/movie/ratings_final.txt'"]}],"metadata":{},"execution_count":10},{"cell_type":"markdown","source":[" # Testing the model"],"metadata":{}},{"cell_type":"markdown","source":[" Separate the preprocessed data"],"metadata":{}},{"source":["\n","\n","train_data = data_info[0]\n","eval_data = data_info[1]\n","test_data = data_info[2]\n","n_entity = data_info[3]\n","n_relation = data_info[4]\n","ripple_set = data_info[5]\n",""],"cell_type":"code","outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'data_info' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-f8a6e781fa65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_entity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_relation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'data_info' is not defined"]}],"metadata":{},"execution_count":11},{"cell_type":"markdown","source":[" # Evaluate"],"metadata":{}},{"source":["\n","\n","model = RippleNet(args, n_entity, n_relation)\n","\n",""],"cell_type":"code","outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'n_entity' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-8655fbd240e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRippleNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_entity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_relation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'n_entity' is not defined"]}],"metadata":{},"execution_count":12},{"source":["\n","\n","# Add ops to save and restore all the variables.\n","saver = tf.train.Saver(max_to_keep=None)\n","\n","sess = tf.Session(config=config)\n","saver = tf.train.import_meta_graph(MODEL_PATH + \".meta\")\n","saver.restore(sess, MODEL_PATH)\n",""],"cell_type":"code","outputs":[{"output_type":"error","ename":"ValueError","evalue":"No variables to save","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-d9db77ab6dad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Add ops to save and restore all the variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[1;32m    823\u001b[0m           time.time() + self._keep_checkpoint_every_n_hours * 3600)\n\u001b[1;32m    824\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[1;32m    860\u001b[0m           \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables to save\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: No variables to save"]}],"metadata":{},"execution_count":13},{"cell_type":"markdown","source":[" ## Custom precision at K eval"],"metadata":{}},{"source":["\n","\n","def generate_truth_dict():\n","\n","    truth_dict = {}\n","    for rating in tqdm(train_data):\n","        user_id, movie_id, score = rating\n","\n","        if user_id not in truth_dict:\n","            truth_dict[user_id] = []\n","\n","        if score == 1:\n","            truth_dict[user_id].append(movie_id)\n","\n","    for rating in tqdm(test_data):\n","        user_id, movie_id, score = rating\n","\n","        if user_id not in truth_dict:\n","            truth_dict[user_id] = []\n","\n","        if score == 1:\n","            truth_dict[user_id].append(movie_id)\n","\n","    for rating in tqdm(eval_data):\n","        user_id, movie_id, score = rating\n","\n","        if user_id not in truth_dict:\n","            truth_dict[user_id] = []\n","\n","        if score == 1:\n","            truth_dict[user_id].append(movie_id)\n","\n","    return truth_dict\n","\n","\n","truth_dict = generate_truth_dict()\n",""],"cell_type":"code","outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'train_data' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-f3687633b3d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtruth_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_truth_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-14-f3687633b3d9>\u001b[0m in \u001b[0;36mgenerate_truth_dict\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtruth_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mrating\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"]}],"metadata":{},"execution_count":14},{"cell_type":"markdown","source":[" ### =============="],"metadata":{}},{"source":["\n","def predict(sess, args, model, users, items):\n","\n","    test_data = _preprocess_test_data(users, items)\n","\n","    scores = []\n","    for i in range(0, len(test_data), args.batch_size):\n","\n","        feed_dict = _get_feed_dict(args, model, test_data, ripple_set, i, i + args.batch_size)\n","        _, _, _, batch_scores, _ = model.custom_eval(sess, feed_dict)\n","        scores = np.concatenate((scores, batch_scores))\n","\n","    return scores\n","\n","def _preprocess_test_data(users, items):\n","    \"Preprocess test data so ripplenet can do feed forward with the right format\"\n","\n","    cust_test_data = []\n","    for user in users:\n","        for item in items:\n","            cust_test_data.append([user, item, 0]) # The last 0 is a dummy value to match input format\n","\n","    return np.array(cust_test_data)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":15},{"source":["\n","\n","def get_suggestion(user, k):\n","\n","    items = [i for i in range(0, 15440)]\n","    prediction = predict(sess, args, model, [user], items)\n","    score_item_pairs = [(prediction[i], i) for i in items]\n","    top_k_recommendations = sorted(score_item_pairs, reverse=True)[:k]\n","\n","    return top_k_recommendations\n","\n","\n","def get_top_truth(user, k):\n","    return truth_dict[user] if user in truth_dict else []\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":16},{"source":["\n","def check_precision(prediction, truth, k=10):\n","    intersect = _get_intersect_pred_truth(pred, truth, k)\n","    len_intersect = len(intersect)\n","    len_truth = len(truth) if 0 < len(truth) <= k else k\n","\n","    return intersect, len_intersect / len_truth\n","\n","def _get_intersect_pred_truth(pred, truth, k):\n","    pred_item_set = {x[1] for x in pred}\n","    truth_item_set = set(truth)\n","\n","    return pred_item_set.intersection(truth_item_set)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":17},{"source":["\n","k_suggestion = 10\n","n_users = 10\n","\n","sample_user = np.random.randint(1, 15000, n_users) # sampling\n","# sample_user = [i in range(0, 15000)] # uncomment to use non sampling\n","\n","suggested_items = []\n","truth_items = []\n","intersects = []\n","scores = []\n","\n","all_intersect = None\n","all_union = None\n","\n","for user in tqdm(sample_user):\n","\n","    try:\n","\n","        top_suggested_items = get_suggestion(sample_user, k_suggestion)\n","        top_truth_items = get_top_truth(sample_user, k_suggestion)\n","\n","        intersect, score = check_precision(top_suggested_items, top_truth_items, k=k_suggestion)\n","\n","        suggested_items.append(top_suggested_items)\n","        truth_items.append(top_truth_items)\n","        intersects.append(intersect)\n","        scores.append(score)\n","\n","        if all_intersect is None:\n","            all_intersect = top_suggested_items\n","        else:\n","            all_intersect = all_intersect.intersection(top_suggested_items)\n","\n","        if all_union is None:\n","            all_union = top_suggested_items\n","        else:\n","            all_union = all_union.union(top_suggested_items)\n","\n","    except Exception as e:\n","        print(\"error occur for {} : {}\".format(user, e))\n","        \n",""],"cell_type":"code","outputs":[{"output_type":"stream","name":"stderr","text":"\r  0%|          | 0/10 [00:00<?, ?it/s]\r100%|██████████| 10/10 [00:00<00:00, 798.96it/s]\n"},{"output_type":"stream","name":"stdout","text":"error occur for 14747 : name 'sess' is not defined\nerror occur for 4783 : name 'sess' is not defined\nerror occur for 9250 : name 'sess' is not defined\nerror occur for 10821 : name 'sess' is not defined\nerror occur for 7550 : name 'sess' is not defined\nerror occur for 6695 : name 'sess' is not defined\nerror occur for 9623 : name 'sess' is not defined\nerror occur for 234 : name 'sess' is not defined\nerror occur for 7166 : name 'sess' is not defined\nerror occur for 5617 : name 'sess' is not defined\n"}],"metadata":{},"execution_count":18},{"source":["print(\"Prec@k score:\", np.average(scores))\n","# print(\"top_suggested_items:\", top_suggested_items)\n","# print(\"truth_items:\", truth_items)\n","\n","print(\"\\nintersect\")\n","print(all_intersect, len(all_intersect))\n","print(\"\\nunion\")\n","print(all_union, len(all_union))\n","print(\"\\ndistinct rate\")\n","print((len(all_union)) / (n_users * k_suggestion))\n",""],"cell_type":"code","outputs":[{"output_type":"stream","name":"stdout","text":"Prec@k score: nan\n\nintersect\n"},{"output_type":"stream","name":"stderr","text":"/home/jessinra/.local/lib/python3.6/site-packages/numpy/lib/function_base.py:390: RuntimeWarning: Mean of empty slice.\n  avg = a.mean(axis)\n/home/jessinra/.local/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n"},{"output_type":"error","ename":"TypeError","evalue":"object of type 'NoneType' has no len()","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-be334e1ee65b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nintersect\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_intersect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_intersect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nunion\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_union\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_union\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"]}],"metadata":{},"execution_count":19},{"source":["\n","sample_user = [np.random.randint(1, 138000) for i in range(0, 3)]\n","\n","for user in sample_user:\n","\n","    prediction = get_suggestion(user, 10)\n","    truth = get_top_truth(user, 10)\n","\n","    display(user)\n","    display((prediction))\n","    display([x[1] for x in prediction])\n","    display((truth))\n","    display(check_precision(prediction, truth, 10))\n","    display(\"==================\")"],"cell_type":"code","outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'sess' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-5c703f9ecda2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_user\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_suggestion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top_truth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-5d6ff71b5137>\u001b[0m in \u001b[0;36mget_suggestion\u001b[0;34m(user, k)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15440\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mscore_item_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtop_k_recommendations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_item_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"]}],"metadata":{},"execution_count":20}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}